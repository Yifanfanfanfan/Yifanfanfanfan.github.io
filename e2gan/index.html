<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="E2GAN Project Page">

  <title>E2GAN: Efficient Training of Efficient GANs for Image-to-Image Translation</title>

  <link href="./static/css/bootstrap.min.css" rel="stylesheet">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <script src="./static/js/index.js"></script>

  <link href="./static/css/font.css" rel="stylesheet" type="text/css">
  <link href="./static/css/style.css" rel="stylesheet" type="text/css">

</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="section">
  <!-- === Title Starts === -->
  <div class="header">
    <!-- <div class="logo">
      <a href="https://mmlab.ie.cuhk.edu.hk/" target="_blank"><img src="./common/cuhk.jfif"></a>
    </div> -->
    <div class="title", style="font-size: 24pt; padding-top: 10pt;">  <!-- Set padding as 10 if title is with two lines. -->
      E<sup>2</sup>GAN: Efficient Training of Efficient GANs for Image-to-Image Translation
		<!-- <br>
		<font color="grey" size="4">arXiv Preprint.</font> -->
    </div>
  </div>
  <!-- === Title Ends === -->
  <div class="author">
    <a href="https://yifanfanfanfan.github.io/">Yifan Gong</a><sup>1,2</sup> &nbsp &nbsp </span>
    <span class="author-block">
      <a href="https://zhanzheng8585.github.io/">Zheng Zhan</a><sup>2</i></sup> &nbsp &nbsp </span>
    <span class="author-block">
      <a href="https://research.snap.com/team/team-member.html#qing-jin">Qing Jin</a><sup>1</sup> &nbsp &nbsp </span>
    <span class="author-block">
      <a href="https://www.linkedin.com/in/yanyu-li-2216aa17b/">Yanyu Li</a><sup>1,2</sup> &nbsp &nbsp </span>
    <span class="author-block">
      <a href="https://www.linkedin.com/in/yerlan-idelbayev/">Yerlan Idelbayev</a><sup>1</sup>&nbsp &nbsp</span>
      <span class="author-block">
        <a href="https://alvinliu0.github.io/">Xian Liu</a><sup>1</sup>&nbsp &nbsp</span>
        <span class="author-block">
          <a href="https://www.linkedin.com/in/asmekal/">Andrey Zharkov</a><sup>1</sup>&nbsp &nbsp</span>
          <span class="author-block">
            <a href="https://kfiraberman.github.io/">Kfir Aberman</a><sup>1</sup>&nbsp &nbsp</span>
            <span class="author-block">
              <a href="http://www.stulyakov.com/">Sergey Tulyakov</a><sup>1</sup>&nbsp &nbsp</span>
              <span class="author-block">
                <a href="https://web.northeastern.edu/yanzhiwang/">Yanzhi Wang</a><sup>2</sup>&nbsp &nbsp</span>
                <span class="author-block">
                  <a href="https://alanspike.github.io/">Jian Ren</a><sup>1</sup></span>
  </div>
  <div class="institution">
    <sup>1</sup>Snap Inc.&nbsp;&nbsp;&nbsp;
    <sup>2</sup>Northeastern University&nbsp;&nbsp;&nbsp;
  </div>

  <div class="column has-text-centered">
    <div class="publication-links">
      <!-- PDF Link. -->
      <span class="link-block">
        <a href=".content/icml_e2gan_arxiv.pdf" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fas fa-file-pdf"></i>
          </span>
          <span>Paper</span>
        </a>
      </span>
      <span class="link-block">
        <a href="https://arxiv.org/abs/2401.06127" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="ai ai-arxiv"></i>
          </span>
          <span>arXiv</span>
        </a>
      </span>
      <!-- Video Link. -->
      <span class="link-block">
        <a href="https://www.youtube.com/watch?v=uGFWVT_qm9Q"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-youtube"></i>
          </span>
          <span> Demo </span>
        </a>
      </span>
      <!-- Code Link. -->
      <span class="link-block">
        <a href="https://github.com/Yifanfanfanfan/Yifanfanfanfan.github.io/tree/main/e2gan"
          class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fab fa-github"></i>
          </span>
          <span>Github</span>
        </a>
      </span>
      <!-- Dataset Link.
      <span class="link-block">
        <a href="./#BibTeX" class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
            <i class="fa fa-quote-left"></i>
          </span>
          <span>Cite</span>
        </a> -->
    </div>

  </div>
  
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./static/images/motivation.jpg" width="90%"></td>
      </tr>
    </table>
    <b> Overview of E<sup>2</sup>GAN.</b> Left: Training Comparison. Conventional GAN training, such as pix2pix [1] and pix2pix-zero-distilled
    that distills Co-Mod-GAN [2] using data from pix2pix-zero [3], requires all the weights trained from scratch, while our efficient training
    significant reduces the training cost by only fine-tuning 1% weights with only portion of training data. Right: Mobile Inference Compari-
    son. Our efficient on-device model can achieve real-time (30FPS, iPhone 14) runtime and is faster than pix2pix and diffusion model, while
    the pix2pix-zero-distilled model (Co-Mod-GAN) is not supported on device.
  </div>
  <!-- <div class="link">
    <a href="https://arxiv.org/pdf/2212.02350.pdf" target="_blank">[Paper]</a>&nbsp;
    <a href="https://github.com/alvinliu0/ANGIE" target="_blank">[Code]</a>&nbsp;
    <a href="https://github.com/alvinliu0/ANGIE" target="_blank">[Dataset]</a>
  </div> -->
  
  </div>
<!-- === Home Section Ends === -->

<!-- === Result Section Starts === -->
<!-- === Result Section Ends === -->
<div class="section">
	<div class="title">Demo Video </div>
	<div class="body">
  
	  We present a short demo video, mostly with quick overview of motivations, our framework design, and visualization results.
  
    <div class="publication-video">
      <iframe src="https://www.youtube.com/embed/uGFWVT_qm9Q" title="YouTube video player"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen></iframe>
    </div>
  </div>
</div>



<!-- === Overview Section Starts === -->
<div class="section">
  <div class="title">Abstract</div>
  <div class="body">
    One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). 
            This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. 
            However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts.
            In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient?
            To achieve this goal, we propose a series of innovative techniques.First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time.
            Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept.
  </div>
  
</div>

<!-- === Overview Section Ends === -->

<div class="section">
  <div class="title">Model Architecture Overview</div>
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./static/images/model_arch.jpg" width="90%"></td>
      </tr>
    </table>
    <b> Overview of E<sup>2</sup>GAN model architecture.</b> The generator is composed of down/up-sampling layers, 3 ResNet Blocks, and 1 Transformer Block. The base generator is trained on multiple representative concepts. New concepts are achieved by fine-tuning LoRA parameters on crucial layers.
  </div>
</div>

<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Qualitative Comparisons on Various Tasks</div>
  <div class="body">
    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <!-- <table width="100%" style="margin: 20pt 0; text-align: center;"> -->
      <div class="container" height=1000px>
        <!-- The leftmost column shows two original images and the remaining columns present the corresponding synthesized images in the target concept domain, where target prompts are shown at the bottom row.  -->
        <div id="results-carousel2" class="carousel2 results-carousel">
          <div class="card-image" style="text-align: center;">
            <img src="./static/images/appendix_1_075.jpg" height="90%">
          </div>
          <div class="card-image" style="text-align: center;">
            <img src="./static/images/appendix_2_075.jpg" height="90%">
          </div>
          <div class="card-image" style="text-align: center;">
            <img src="./static/images/appendix_3_075.jpg" height="90%">
          </div>
          <div class="card-image" style="text-align: center;">
            <img src="./static/images/appendix_4_075.jpg" height="90%">
          </div>
          <div class="card-image" style="text-align: center;">
            <img src="./static/images/experiment_example.jpg" height="90%">
          </div>
          <div class="card-image" style="text-align: center;">
            <img src="./static/images/appendix_5_075.jpg" height="90%">
          </div>
        </div>
      </div>
    <!-- </table> -->
  
  </div>
</div>
<!-- === Result Section Ends === -->



<!-- === Result Section Starts === -->
<div class="section">
  <div class="title">Quantitative Results</div>
  <div class="body">

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./static/images/experimental_results.jpg" width="45%"></td>
      </tr>
    </table>
    <b>FID comparison</b>. FID is calculated between the images generated by GAN-based approaches and diffusion models. Reported FID is averaged across different concepts (30 for FFHQ and 10 for Flicker Scenery).
    <br>
    <br>
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <td><img src="./static/images/compare.png" width="80%"></td>
      </tr>
    </table>
    <b>Analysis (FID) of various base models </b> on FFHQ. <b>Left</b>: Training FLOPs. <b>Middle</b>: Training time. <b>Right</b>: Number of parameters that required gradient update, which also equals to the weights need to be saved for a concept.
    <br>
    <br>
    <table width="100%" style="margin: 20pt 0; text-align: center;">
      <tr>
        <table><tr>
          <td><img src="./static/images/base_model_compare.jpg" width="80%" border=0></td>
          <td><img src="./static/images/lora_rank_compare.jpg" width="75%" border=0></td>
          </tr></table>
      </tr>
    </table>
    <b>Left</b>: Analysis (FID) of various base models on FFHQ. <b>Right</b>: Analysis of searching LoRA rank on the Flicker Scenery dataset. The reported FID values are averaged over 10 different target concepts.
  </div>
</div>
<!-- === Result Section Ends === -->



<!-- === Result Section Starts === -->
<!-- <div class="section">
  <div class="title">More Qualitative Results (1024x1024)</div>
  <div class="body"> -->

    <!-- Adjust the number of rows and columns (EVERY project differs). -->
    <!-- <table width="100%" style="margin: 20pt 0; text-align: center;">
      <div class="container" width=1000px>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/our_more1.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/our_more2.png" width="90%">
          </div>
          <div class="item item-clevr" style="text-align: center;">
            <img src="./content/our_more3.png" width="90%">
          </div>
        </div>
      </div> -->

      <!-- <tr>
        <td><img src="./content/our_more1.png" width="90%"></td>
      </tr> -->
    <!-- </table>
  
  </div>
</div> -->
<!-- === Result Section Ends === -->

  <!-- Reference. -->
  <div class="section">
    <h2 class="title" style="text-align: center;">Reference</h2>
    <p>[<a href="https://arxiv.org/pdf/1611.07004.pdf">1</a>]&nbsp;Image-to-Image Translation with Conditional Adversarial Networks</p>
    <p>[<a href="https://arxiv.org/pdf/2103.10428.pdf">2</a>]&nbsp;Large Scale Image Completion via Co-Modulated Generative Adversarial Networks</p>
    <p>[<a href="https://pix2pixzero.github.io/">3</a>]&nbsp;Zero-shot Image-to-Image Translation</p>
  </div>
  <!--/ Reference. -->

<!-- === Reference Section Starts === -->
<div class="section">
  <div class="bibtex">BibTeX</div>
<pre>
@article{gong20242,
  title={E $\^{}$\{$2$\}$ $ GAN: Efficient Training of Efficient GANs for Image-to-Image Translation},
  author={Gong, Yifan and Zhan, Zheng and Jin, Qing and Li, Yanyu and Idelbayev, Yerlan and Liu, Xian and Zharkov, Andrey and Aberman, Kfir and Tulyakov, Sergey and Wang, Yanzhi and others},
  journal={arXiv preprint arXiv:2401.06127},
  year={2024}
}
</pre>


</body>
</html>
